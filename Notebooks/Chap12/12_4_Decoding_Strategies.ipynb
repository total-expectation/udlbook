{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPsZjfqVeHYh95Hzt+hCIO7",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap12/12_4_Decoding_Strategies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Notebook 12.4: Decoding strategies**\n",
    "\n",
    "This practical investigates neural decoding from transformer models.  \n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
    "\n",
    "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."
   ],
   "metadata": {
    "id": "RnIUiieJWu6e"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:30.291638100Z",
     "start_time": "2024-05-17T02:30:30.251623400Z"
    }
   },
   "execution_count": 160
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, set_seed\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "id": "sMOyD0zem2Ef",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:30.305198800Z",
     "start_time": "2024-05-17T02:30:30.295679600Z"
    }
   },
   "execution_count": 161,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ],
   "metadata": {
    "id": "pZgfxbzKWNSR",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:31.937617200Z",
     "start_time": "2024-05-17T02:30:30.299197600Z"
    }
   },
   "execution_count": 162,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decoding from GPT2\n",
    "\n",
    "This tutorial investigates how to use GPT2 (the forerunner of GPT3) to generate text.  There are a number of ways to do this that trade-off the realism of the text against the amount of variation.\n",
    "\n",
    "At every stage, GPT2 takes an input string and returns a probability for each of the possible subsequent tokens.  We can choose what to do with these probability.  We could always *greedily choose* the most likely next token, or we could draw a *sample* randomly according to the probabilities.  There are also intermediate strategies such as *top-k sampling* and *nucleus sampling*, that have some controlled randomness.\n",
    "\n",
    "We'll also investigate *beam search* -- the idea is that rather than greedily take the next best token at each stage, we maintain a set of hypotheses  (beams)as we add each subsequent token and return the most likely overall hypothesis.  This is not necessarily the same result we get from greedily choosing the next token."
   ],
   "metadata": {
    "id": "TfhAGy0TXEvV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's investigate the token themselves.  The code below prints out the vocabulary size and shows 20 random tokens.  "
   ],
   "metadata": {
    "id": "vsmO9ptzau3_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "print(\"Number of tokens in dictionary = %d\"%(tokenizer.vocab_size))\n",
    "for i in range(20):\n",
    "  index = np.random.randint(tokenizer.vocab_size)\n",
    "  print(\"Token: %d \"%(index)+tokenizer.decode(torch.tensor(index), skip_special_tokens=True))\n"
   ],
   "metadata": {
    "id": "dmmBNS5GY_yk",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:31.939615600Z",
     "start_time": "2024-05-17T02:30:31.909944200Z"
    }
   },
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dictionary = 50257\n",
      "Token: 33003  Mormons\n",
      "Token: 12172  cam\n",
      "Token: 5192  trig\n",
      "Token: 32511 ojure\n",
      "Token: 50057  gist\n",
      "Token: 43723  Petition\n",
      "Token: 7813  sin\n",
      "Token: 21440  Witness\n",
      "Token: 32912  Remy\n",
      "Token: 20609 isure\n",
      "Token: 49100  creeps\n",
      "Token: 7751  fasc\n",
      "Token: 43757  Alc\n",
      "Token: 31228  messenger\n",
      "Token: 36230  SYSTEM\n",
      "Token: 32025  precipitation\n",
      "Token: 21758  cores\n",
      "Token: 45413  Forestry\n",
      "Token: 35730  guru\n",
      "Token: 8444  Disc\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sampling\n",
    "\n",
    "Each time we run GPT2 it will take in a set of tokens, and return a probability over each of the possible next tokens.  The simplest thing we could do is to just draw a sample from this probability distribution each time."
   ],
   "metadata": {
    "id": "MUM3kLEjbTso"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def sample_next_token(input_tokens, model, tokenizer):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "  # TODO Draw a random token according to the probabilities\n",
    "  # next_token should be an array with an sole integer in it (as below)\n",
    "  # Use:  https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "  # Replace this line\n",
    "  # so the solution was to use tokenizer, it has all the indices for the tokens, we sample from this one using the probability distribution of prob_over_tokens\n",
    "  # specifically it's just the index of the token that is being drawn\n",
    "  next_token = np.random.choice(tokenizer.vocab_size, p=prob_over_tokens)\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  # output_tokens['input_ids'] is 2D with shape (1,6) so next_token need to be 2D as well\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([[next_token]])),dim=1)\n",
    "  # output_tokens['attention_mask'] is 2D with shape (1,6), so [[1]] matches the dimension and is concatenated along the col making it shape (1,7)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]  # probability of our next_token\n",
    "\n",
    "  return output_tokens"
   ],
   "metadata": {
    "id": "TIyNgg0FkJKO",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:31.939615600Z",
     "start_time": "2024-05-17T02:30:31.919097800Z"
    }
   },
   "execution_count": 164,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Expected output:\n",
    "# \"The best thing about Bath is that they don't even change or shrink anymore.\"\n",
    "\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(10):\n",
    "    input_tokens = sample_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))\n",
    "\n"
   ],
   "metadata": {
    "id": "BHs-IWaz9MNY",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:32.818342500Z",
     "start_time": "2024-05-17T02:30:31.923614Z"
    }
   },
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is that\n",
      "The best thing about Bath is that they\n",
      "The best thing about Bath is that they don\n",
      "The best thing about Bath is that they don't\n",
      "The best thing about Bath is that they don't even\n",
      "The best thing about Bath is that they don't even change\n",
      "The best thing about Bath is that they don't even change or\n",
      "The best thing about Bath is that they don't even change or shrink\n",
      "The best thing about Bath is that they don't even change or shrink anymore\n",
      "The best thing about Bath is that they don't even change or shrink anymore.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO Modify the code below by changing the number of tokens generated and the initial sentence\n",
    "# to get a feel for how well this works.  Since I didn't reset the seed, it will give a different\n",
    "# answer every time that you run it.\n",
    "\n",
    "# TODO Experiment with changing this line:\n",
    "input_txt = \"The best thing about Bath\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "# TODO Experiment with changing this line:\n",
    "for i in range(20):\n",
    "    input_tokens = sample_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ],
   "metadata": {
    "id": "yN98_7WqbvIe",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:34.764359100Z",
     "start_time": "2024-05-17T02:30:32.821341400Z"
    }
   },
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bathurst\n",
      "The best thing about Bathurst is\n",
      "The best thing about Bathurst is that\n",
      "The best thing about Bathurst is that everyone\n",
      "The best thing about Bathurst is that everyone is\n",
      "The best thing about Bathurst is that everyone is as\n",
      "The best thing about Bathurst is that everyone is as \"\n",
      "The best thing about Bathurst is that everyone is as \"impro\n",
      "The best thing about Bathurst is that everyone is as \"improved\n",
      "The best thing about Bathurst is that everyone is as \"improved as\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy Domin\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy Dominick\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy Dominick before\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy Dominick before the\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy Dominick before the bike\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy Dominick before the bike was\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy Dominick before the bike was rigged\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy Dominick before the bike was rigged up\n",
      "The best thing about Bathurst is that everyone is as \"improved as Randy Dominick before the bike was rigged up.\"\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Greedy token selection\n",
    "\n",
    "You probably (correctly) got the impression that the text from pure sampling of the probability model can be kind of random.  How about if we choose most likely token at each step?\n"
   ],
   "metadata": {
    "id": "7eHFLCeZcmmg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_best_next_token(input_tokens, model, tokenizer):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # TODO -- find the token index with the maximum probability\n",
    "  # It should be returns as a list (i.e., put squared brackets around it)\n",
    "  # Use https://numpy.org/doc/stable/reference/generated/numpy.argmax.html\n",
    "  # Replace this line\n",
    "  # specifically it's just the index of the token that is being drawn, argmax returns the index of the maximum value\n",
    "  next_token = np.argmax(prob_over_tokens)\n",
    "\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([[next_token]])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  return output_tokens"
   ],
   "metadata": {
    "id": "OhRzynEjxpZF",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:34.771359800Z",
     "start_time": "2024-05-17T02:30:34.768360500Z"
    }
   },
   "execution_count": 167,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Expected output:\n",
    "# The best thing about Bath is that it's a place where you can go to\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(10):\n",
    "    input_tokens = get_best_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ],
   "metadata": {
    "id": "gKB1Mgndj-Hm",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:35.683357Z",
     "start_time": "2024-05-17T02:30:34.773359Z"
    }
   },
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is that\n",
      "The best thing about Bath is that it\n",
      "The best thing about Bath is that it's\n",
      "The best thing about Bath is that it's a\n",
      "The best thing about Bath is that it's a place\n",
      "The best thing about Bath is that it's a place where\n",
      "The best thing about Bath is that it's a place where you\n",
      "The best thing about Bath is that it's a place where you can\n",
      "The best thing about Bath is that it's a place where you can go\n",
      "The best thing about Bath is that it's a place where you can go to\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO Modify the code below by changing the number of tokens generated and the initial sentence\n",
    "# to get a feel for how well this works.\n",
    "\n",
    "# TODO Experiment with changing this line:\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "# TODO Experiment with changing this line:\n",
    "for i in range(20):\n",
    "    input_tokens = get_best_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ],
   "metadata": {
    "id": "L1YHKaYFfC0M",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:37.675873700Z",
     "start_time": "2024-05-17T02:30:35.685923400Z"
    }
   },
   "execution_count": 169,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is that\n",
      "The best thing about Bath is that it\n",
      "The best thing about Bath is that it's\n",
      "The best thing about Bath is that it's a\n",
      "The best thing about Bath is that it's a place\n",
      "The best thing about Bath is that it's a place where\n",
      "The best thing about Bath is that it's a place where you\n",
      "The best thing about Bath is that it's a place where you can\n",
      "The best thing about Bath is that it's a place where you can go\n",
      "The best thing about Bath is that it's a place where you can go to\n",
      "The best thing about Bath is that it's a place where you can go to get\n",
      "The best thing about Bath is that it's a place where you can go to get a\n",
      "The best thing about Bath is that it's a place where you can go to get a drink\n",
      "The best thing about Bath is that it's a place where you can go to get a drink and\n",
      "The best thing about Bath is that it's a place where you can go to get a drink and have\n",
      "The best thing about Bath is that it's a place where you can go to get a drink and have a\n",
      "The best thing about Bath is that it's a place where you can go to get a drink and have a good\n",
      "The best thing about Bath is that it's a place where you can go to get a drink and have a good time\n",
      "The best thing about Bath is that it's a place where you can go to get a drink and have a good time.\n",
      "The best thing about Bath is that it's a place where you can go to get a drink and have a good time. It\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Top-K sampling\n",
    "\n",
    "You probably noticed that the greedy strategy produces quite realistic text, but it's kind of boring.  It produces generic answers.  Also, if this was a chatbot, then we wouldn't necessarily want it to produce the same answer to a question each time.  \n",
    "\n",
    "Top-K sampling is a compromise strategy that samples randomly from the top K most probable tokens.  We could just choose them with a uniform distribution, or (as here) we could sample them according to their original probabilities."
   ],
   "metadata": {
    "id": "1ORFXYX_gBDT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_top_k_token(input_tokens, model, tokenizer, k=20):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # Draw a sample from the top K most likely tokens.\n",
    "  # Take copy of the probabilities and sort from largest to smallest (use np.sort)\n",
    "  # TODO -- replace this line\n",
    "  sorted_k_top_indices = np.argsort(prob_over_tokens[np.argpartition(prob_over_tokens, -k)[-k:]])\n",
    "  sorted_prob_over_tokens = prob_over_tokens[sorted_k_top_indices]\n",
    "\n",
    "  # Find the probability at the k'th position\n",
    "  # TODO -- replace this line\n",
    "  kth_prob_value = sorted_prob_over_tokens[k-1]  # zero-indexed\n",
    "\n",
    "  # Set all probabilities below this value to zero\n",
    "  prob_over_tokens[prob_over_tokens<kth_prob_value] = 0\n",
    "\n",
    "  # Renormalize the probabilities so that they sum to one\n",
    "  # TODO -- replace this line\n",
    "  prob_over_tokens = prob_over_tokens / np.sum(prob_over_tokens)\n",
    "\n",
    "\n",
    "  # Draw random token\n",
    "  # specifically it's just the index of the token that is being drawn\n",
    "  next_token = np.random.choice(len(prob_over_tokens), 1, replace=False, p=prob_over_tokens)\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  return output_tokens"
   ],
   "metadata": {
    "id": "7RFbn6c-0Z4v",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:37.684873200Z",
     "start_time": "2024-05-17T02:30:37.680874Z"
    }
   },
   "execution_count": 170,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Expected output:\n",
    "# The best thing about Bath is that you get to see all the beautiful faces of\n",
    "\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(10):\n",
    "    input_tokens = get_top_k_token(input_tokens, model, tokenizer, k=10)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ],
   "metadata": {
    "id": "G3w1GVED4HYv",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:38.600953400Z",
     "start_time": "2024-05-17T02:30:37.688873400Z"
    }
   },
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is that\n",
      "The best thing about Bath is that not\n",
      "The best thing about Bath is that not only\n",
      "The best thing about Bath is that not only do\n",
      "The best thing about Bath is that not only do you\n",
      "The best thing about Bath is that not only do you get\n",
      "The best thing about Bath is that not only do you get to\n",
      "The best thing about Bath is that not only do you get to tip\n",
      "The best thing about Bath is that not only do you get to tip places\n",
      "The best thing about Bath is that not only do you get to tip places that\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "# Experiment with different values of k\n",
    "# If you set it to a lower number (say 3) the text will be less random\n",
    "# If you set it to a higher number (say 5000) the text will be more random\n",
    "\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(20):\n",
    "    input_tokens = get_top_k_token(input_tokens, model, tokenizer, k=5000)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ],
   "metadata": {
    "id": "RySu2bzqpW9E",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:40.697475Z",
     "start_time": "2024-05-17T02:30:38.600953400Z"
    }
   },
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing about Bath is that\n",
      "The best thing about Bath is that they\n",
      "The best thing about Bath is that they don\n",
      "The best thing about Bath is that they don't\n",
      "The best thing about Bath is that they don't even\n",
      "The best thing about Bath is that they don't even change\n",
      "The best thing about Bath is that they don't even change it\n",
      "The best thing about Bath is that they don't even change it often\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "They\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "They've\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "They've transformed\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "They've transformed the\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "They've transformed the call\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "They've transformed the call-\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "They've transformed the call-sign\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "They've transformed the call-sign outside\n",
      "The best thing about Bath is that they don't even change it often!\"\n",
      "\n",
      "They've transformed the call-sign outside White\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Nucleus sampling\n",
    "\n",
    "Top-K sampling has the disadvantage that sometimes there are only a few plausible next tokens, and sometimes there are a lot. An example of this is answering a yes or no question as opposed to answering an open-ended question, in these situations the k is fixed and can't adapt.  How do we adapt to this situation?  One way is to sample from a fixed proportion of the probability mass.  That is we order the tokens in terms of probability and cut off the possibility of sampling when the cumulative sum is greater than a threshold.\n",
    "\n",
    "This way, we adapt the number of possible tokens that we can choose.\n",
    "\n",
    "<span style=\"color:green;white-space:pre-wrap\">According to this blog [SAMPLING TECHNIQUES LLM](https://huyenchip.com/2024/01/16/sampling.html#top_p) it takes from descending order until the threshold is met. But doesn't explain what happens if the very first element exceeds the threshold, do you just skip it or take it?</span>\n"
   ],
   "metadata": {
    "id": "fOHak_QJfU-2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_nucleus_sampling_token(input_tokens, model, tokenizer, thresh=0.25):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "  # Find the most likely tokens that make up the first (thresh) of the probability, according to the sampling techniques link it's the fewest tokens that exceeds the threshold\n",
    "  # so we are looking at <=\n",
    "  # TODO -- sort the probabilities in decreasing order\n",
    "  # Replace this line\n",
    "  sorted_probs_decreasing = np.sort(prob_over_tokens)[::-1]\n",
    "  # TODO -- compute the cumulative sum of these probabilities\n",
    "  # Replace this line\n",
    "  cum_sum_probs = np.cumsum(sorted_probs_decreasing)\n",
    "  # Find index where the cumulative sum is greater than the threshold\n",
    "  # because the array will only contain false or truth values, then np.argmax will pick the first truth value in the list, which is equivalent to the first time the cumsum exceeds the threshold\n",
    "  # and this is exactly what we are looking for\n",
    "  thresh_index = np.argmax(cum_sum_probs>thresh)\n",
    "  print(\"Choosing from %d tokens\"%(thresh_index))\n",
    "  # TODO:  Find the probability value to threshold\n",
    "  # Replace this line:\n",
    "  thresh_prob = cum_sum_probs[thresh_index]\n",
    "  # Set any probabilities less than this to zero\n",
    "  # Correction, I think this is plain wrong. For instance if all the probabilities are less than the threshold probability, \n",
    "  # then we will always get zero everywhere. A better way is to keep the first probabilities according to the thresh_index \n",
    "  # and the rest set to zero, because they aren't considered\n",
    "  prob_over_tokens[prob_over_tokens<thresh_prob] = 0\n",
    "  # prob_over_tokens[thresh_index + 1:] = 0\n",
    "  # Renormalize\n",
    "  prob_over_tokens = prob_over_tokens / thresh_prob\n",
    "  # Draw random token\n",
    "  next_token = np.random.choice(len(prob_over_tokens), 1, replace=False, p=prob_over_tokens)\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  return output_tokens"
   ],
   "metadata": {
    "id": "PtxS4kNDyUcm",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:40.705475800Z",
     "start_time": "2024-05-17T02:30:40.702476100Z"
    }
   },
   "execution_count": 173,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Expected output:\n",
    "# The best thing about Bath is that it's not a city that has been around\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(10):\n",
    "    input_tokens = get_nucleus_sampling_token(input_tokens, model, tokenizer, thresh = 0.2)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))\n"
   ],
   "metadata": {
    "id": "K2Vk1Ly40S6c",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:41.089019300Z",
     "start_time": "2024-05-17T02:30:40.707475300Z"
    }
   },
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing from 0 tokens\n",
      "The best thing about Bath is that\n",
      "Choosing from 0 tokens\n",
      "The best thing about Bath is that it\n",
      "Choosing from 0 tokens\n",
      "The best thing about Bath is that it's\n",
      "Choosing from 2 tokens\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[174], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m input_tokens \u001B[38;5;241m=\u001B[39m tokenizer(input_txt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m----> 7\u001B[0m     input_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mget_nucleus_sampling_token\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthresh\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mdecode(input_tokens[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n",
      "Cell \u001B[1;32mIn[173], line 31\u001B[0m, in \u001B[0;36mget_nucleus_sampling_token\u001B[1;34m(input_tokens, model, tokenizer, thresh)\u001B[0m\n\u001B[0;32m     29\u001B[0m prob_over_tokens \u001B[38;5;241m=\u001B[39m prob_over_tokens \u001B[38;5;241m/\u001B[39m thresh_prob\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Draw random token\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m next_token \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprob_over_tokens\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprob_over_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Append token to sentence\u001B[39;00m\n\u001B[0;32m     34\u001B[0m output_tokens \u001B[38;5;241m=\u001B[39m input_tokens\n",
      "File \u001B[1;32mnumpy\\\\random\\\\mtrand.pyx:975\u001B[0m, in \u001B[0;36mnumpy.random.mtrand.RandomState.choice\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: probabilities do not sum to 1"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO -- experiment with setting the threshold probability to larger or smaller values\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(10):\n",
    "    input_tokens = get_nucleus_sampling_token(input_tokens, model, tokenizer, thresh = 0.25)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ],
   "metadata": {
    "id": "eQNNHe14wDvC",
    "ExecuteTime": {
     "start_time": "2024-05-17T02:30:41.092019200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Beam search\n",
    "\n",
    "All of the methods we've seen so far choose the tokens one by one.  But this isn't necessarily sensible.  Even greedily choosing the best token doesn't necessarily retrieve the sequence with the highest probability.  It might be that the most likely token only has very unlikely tokens following it.\n",
    "\n",
    "Beam search maintains $K$ hypotheses about the best possible continuation.  It starts with the top $K$ continuations.  Then for each of those, it finds the top K continuations, giving $K^2$ hypotheses.  Then it retains just the top $K$ of these so that the number of hypotheses stays the same."
   ],
   "metadata": {
    "id": "WMMNeLixwlgM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# This routine returns the k'th most likely next token.\n",
    "# If k =0 then it returns the most likely token, if k=1 it returns the next most likely and so on\n",
    "# We will need this for beam search\n",
    "def get_kth_most_likely_token(input_tokens, model, tokenizer, k):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # Find the k'th most likely token\n",
    "  # TODO Sort the probabilities from largest to smallest\n",
    "  # Replace this line:\n",
    "  sorted_prob_over_tokens = np.sort(prob_over_tokens)[::-1]\n",
    "  # TODO Find the k'th sorted probability\n",
    "  # Replace this line\n",
    "  kth_prob_value = sorted_prob_over_tokens[k]\n",
    "\n",
    "\n",
    "\n",
    "  # Find position of this token.\n",
    "  next_token = np.where(prob_over_tokens == kth_prob_value)[0]\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  output_tokens['log_prob'] = output_tokens['log_prob'] + np.log(prob_over_tokens[next_token])\n",
    "  return output_tokens"
   ],
   "metadata": {
    "id": "sAI2bClXCe2F",
    "ExecuteTime": {
     "start_time": "2024-05-17T02:30:41.093018300Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# We can test this code and see that if we choose the 2nd most likely (K=1) token each time\n",
    "# then we get much better generation results than if we choose the 2001st most likely token\n",
    "\n",
    "# Expected output:\n",
    "# The best thing about Bath is the way you get the most bang outta the\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "input_tokens['log_prob'] = 0.0\n",
    "for i in range(10):\n",
    "    input_tokens = get_kth_most_likely_token(input_tokens, model, tokenizer, k=1)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))\n",
    "\n",
    "# Expected output:\n",
    "# The best thing about Bath is mixed profits partnershipsÂ» buy generic+ Honda throttlecont\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "input_tokens['log_prob'] = 0.0\n",
    "for i in range(10):\n",
    "    input_tokens = get_kth_most_likely_token(input_tokens, model, tokenizer, k=2000)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))\n",
    "\n",
    "# TODO -- play around with different values of K"
   ],
   "metadata": {
    "id": "6kSc0WrTELMd",
    "ExecuteTime": {
     "start_time": "2024-05-17T02:30:41.096018200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Print out each beam plus the log probability\n",
    "def print_beams(beams):\n",
    "  for index,beam in enumerate(beams):\n",
    "    print(\"Beam %d, Prob %3.3f: \"%(index,beam['log_prob'])+tokenizer.decode(beam[\"input_ids\"][0], skip_special_tokens=True))\n",
    "  print('---')\n",
    "\n",
    "\n",
    "# TODO:  Read this code carefully!\n",
    "def do_beam_search(input_tokens_in, model, tokenizer, n_beam=5, beam_length=10):\n",
    "  # Store beams in a list\n",
    "  input_tokens['log_prob'] = 0.0\n",
    "\n",
    "  # Initialize with n_beam most likely continuations\n",
    "  beams = [None] * n_beam\n",
    "  for c_k in range(n_beam):\n",
    "    beams[c_k] = dict(input_tokens_in)\n",
    "    beams[c_k] = get_kth_most_likely_token(beams[c_k], model, tokenizer, c_k)\n",
    "\n",
    "  print_beams(beams)\n",
    "\n",
    "  # For each token in the sequence we will add\n",
    "  for c_pos in range(beam_length-1):\n",
    "    # Now for each beam, we continue it in the most likely ways, making n_beam*n_beam type hypotheses\n",
    "    beams_all = [None] * (n_beam*n_beam)\n",
    "    log_probs_all = np.zeros(n_beam*n_beam)\n",
    "    # For each current hypothesis\n",
    "    for c_beam in range(n_beam):\n",
    "      # For each continuation\n",
    "      for c_k in range(n_beam):\n",
    "        # Store the continuation and the probability\n",
    "        beams_all[c_beam * n_beam + c_k] = dict(get_kth_most_likely_token(beams[c_beam], model, tokenizer, c_k))\n",
    "        log_probs_all[c_beam * n_beam + c_k] = beams_all[c_beam * n_beam + c_k]['log_prob']\n",
    "\n",
    "    # Keep the best n_beams sequences with the highest probabilities\n",
    "    sorted_index = np.argsort(np.array(log_probs_all)*-1)\n",
    "    for c_k in range(n_beam):\n",
    "      beams[c_k] = dict(beams_all[sorted_index[c_k]])\n",
    "\n",
    "    # Print the beams\n",
    "    print_beams(beams)\n",
    "\n",
    "  return beams[0]"
   ],
   "metadata": {
    "id": "Y4hFfwPFFxka",
    "ExecuteTime": {
     "start_time": "2024-05-17T02:30:41.097017700Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Expected output:\n",
    "# The best thing about Bath is that it's a place where you don't have to\n",
    "\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about Bath is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "\n",
    "# Now let's call the beam search\n",
    "# It takes a while as it has to run the model multiple times to add a token\n",
    "n_beams = 5\n",
    "best_beam = do_beam_search(input_tokens,model,tokenizer)\n",
    "print(\"Beam search result:\")\n",
    "print(tokenizer.decode(best_beam[\"input_ids\"][0], skip_special_tokens=True))\n",
    "\n",
    "# You should see that the best answer is not the same as the greedy solution we found above\n"
   ],
   "metadata": {
    "id": "0YWKwZmz4NXb",
    "ExecuteTime": {
     "start_time": "2024-05-17T02:30:41.099018100Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can read about more decoding strategies in this blog (which uses a recursive neural network, not a transformer, but the principles are the same).\n",
    "\n",
    "https://www.borealisai.com/research-blogs/tutorial-6-neural-natural-language-generation-decoding-algorithms/\n",
    "\n",
    "You can also look at other possible language models via hugging face:\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.25.1/en/model_summary#decoders-or-autoregressive-models\n"
   ],
   "metadata": {
    "id": "-SXpjZPYsMhv"
   }
  }
 ]
}
