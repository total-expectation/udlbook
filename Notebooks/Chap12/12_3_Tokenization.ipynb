{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP0/KodWM9Dtr2x+8MdXXH1",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap12/12_3_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Notebook 12.3: Tokenization**\n",
    "\n",
    "This notebook builds set of tokens from a text string as in figure 12.8 of the book.\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
    "\n",
    "I adapted this code from *SOMEWHERE*.  If anyone recognizes it, can you let me know and I will give the proper attribution or rewrite if the license is not permissive.\n",
    "\n",
    "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
    "\n"
   ],
   "metadata": {
    "id": "t9vk9Elugvmi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re, collections\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ],
   "metadata": {
    "id": "3_WkaFO3OfLi",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.832418400Z",
     "start_time": "2024-05-17T02:30:27.828404700Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"a sailor went to sea sea sea \"+\\\n",
    "                  \"to see what he could see see see \"+\\\n",
    "                  \"but all that he could see see see \"+\\\n",
    "                  \"was the bottom of the deep blue sea sea sea\""
   ],
   "metadata": {
    "id": "tVZVuauIXmJk",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.837116200Z",
     "start_time": "2024-05-17T02:30:27.832418400Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenize the input sentence To begin with the tokens are the individual letters and the </w> whitespace token. So, we represent each word in terms of these tokens with spaces between the tokens to delineate them.\n",
    "\n",
    "The tokenized text is stored in a structure that represents each word as tokens together with the count of how often that word occurs.  We'll call this the *vocabulary*."
   ],
   "metadata": {
    "id": "fF2RBrouWV5w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def initialize_vocabulary(text):\n",
    "  vocab = collections.defaultdict(int)\n",
    "  words = text.strip().split()\n",
    "  for word in words:\n",
    "      # note that ' '.join() means pad with white space between the characters, list here is the same as splitting up the words into characters try for instance list(\"hello\") to see what I mean\n",
    "      vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "  return vocab"
   ],
   "metadata": {
    "id": "OfvXkLSARk4_",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.860057600Z",
     "start_time": "2024-05-17T02:30:27.837116200Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab = initialize_vocabulary(text)\n",
    "print('Vocabulary: {}'.format(vocab))\n",
    "print('Size of vocabulary: {}'.format(len(vocab)))"
   ],
   "metadata": {
    "id": "aydmNqaoOpSm",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.861056600Z",
     "start_time": "2024-05-17T02:30:27.841114400Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: defaultdict(<class 'int'>, {'a </w>': 1, 's a i l o r </w>': 1, 'w e n t </w>': 1, 't o </w>': 2, 's e a </w>': 6, 's e e </w>': 7, 'w h a t </w>': 1, 'h e </w>': 2, 'c o u l d </w>': 2, 'b u t </w>': 1, 'a l l </w>': 1, 't h a t </w>': 1, 'w a s </w>': 1, 't h e </w>': 2, 'b o t t o m </w>': 1, 'o f </w>': 1, 'd e e p </w>': 1, 'b l u e </w>': 1})\n",
      "Size of vocabulary: 18\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Why create tokens like this i.e. padding each letter with space?\n",
    "<span style=\"color:green;white-space:pre-wrap\">One might ask why we create the tokens this way, by adding white spaces between each character. It seems that later on they want to know the frequencies of each character and also what other character follows it, so it makes it easier to look into that when the tokens are constructed this way.</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find all single letter frequencies\n",
    "Find all the tokens in the current vocabulary and their frequencies.\n",
    "<span style=\"color:green;white-space:pre-wrap\">Interpretation, the less white spaces there are in the keys of the vocab, the fewer number of tokens. This happens when merging tokens together, always looking for adjacent pairs. So as the iteration increases in the merging process, the fewer tokens there will be that have sub-words or letters in them that are split by whitespace.</span>"
   ],
   "metadata": {
    "id": "fJAiCjphWsI9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_tokens_and_frequencies(vocab):\n",
    "  tokens = collections.defaultdict(int)\n",
    "  for word, freq in vocab.items():\n",
    "      word_tokens = word.split()\n",
    "      for token in word_tokens:\n",
    "          tokens[token] += freq\n",
    "  return tokens"
   ],
   "metadata": {
    "id": "qYi6F_K3RYsW",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.861056600Z",
     "start_time": "2024-05-17T02:30:27.846812800Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokens = get_tokens_and_frequencies(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))"
   ],
   "metadata": {
    "id": "Y4LCVGnvXIwp",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.862057400Z",
     "start_time": "2024-05-17T02:30:27.850353500Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: defaultdict(<class 'int'>, {'a': 12, '</w>': 33, 's': 15, 'i': 1, 'l': 6, 'o': 8, 'r': 1, 'w': 3, 'e': 28, 'n': 1, 't': 11, 'h': 6, 'c': 2, 'u': 4, 'd': 3, 'b': 3, 'm': 1, 'f': 1, 'p': 1})\n",
      "Number of tokens: 19\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find adjacent pair (two letters adjacent to each other) frequency\n",
    "Find each pair of adjacent tokens in the vocabulary\n",
    "and count them.  We will subsequently merge the most frequently occurring pair.\n",
    "\n",
    "<span style=\"color:green;white-space:pre-wrap\">It's not apparent, but what the split does it to split at white characters. But that means tokens such as 's e e' and 's e a' will add more to the frequency of ('s', e'), meaning that ('s', 'e') will have contributions of more than just one token. This explains why using the initial vocab to create pairs we have as much as 13 ('s', 'e') pairs. Note that there's an edge case here, if there are no white spaces within the keys of the vocab, i.e. we have converged by merging everything, then the word.split() will return just the word. But the loop will see that the len(word) is just 1 because you got one word in a list and so subtracting that with 1 gives zero. This will happen to every key now that everything has been merged, so we will only return an empty pair dictionary. Later on this will cause issue when we try to take max frequency of this pair dictionary. That's what is causing the error.</span>"
   ],
   "metadata": {
    "id": "_-Rh1mD_Ww3b"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_pairs_and_counts(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        # so this splits each key of the vocab by the white space, meaning that keys that have overlapping sub-words will contribute much more to those pairs\n",
    "        # in this case 's e e' and 's e a' will contribute to the frequency of the pair ('s', 'e')\n",
    "        # anyway, any pair will be constructed given a key as long as there are white spaces between the characters in the key\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ],
   "metadata": {
    "id": "OqJTB3UFYubH",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.862793Z",
     "start_time": "2024-05-17T02:30:27.856141Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pairs = get_pairs_and_counts(vocab)\n",
    "print('Pairs: {}'.format(pairs))\n",
    "print('Number of distinct pairs: {}'.format(len(pairs)))\n",
    "\n",
    "most_frequent_pair = max(pairs, key=pairs.get)\n",
    "print('Most frequent pair: {}'.format(most_frequent_pair))"
   ],
   "metadata": {
    "id": "d-zm0JBcZSjS",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.864788400Z",
     "start_time": "2024-05-17T02:30:27.861056600Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs: defaultdict(<class 'int'>, {('a', '</w>'): 7, ('s', 'a'): 1, ('a', 'i'): 1, ('i', 'l'): 1, ('l', 'o'): 1, ('o', 'r'): 1, ('r', '</w>'): 1, ('w', 'e'): 1, ('e', 'n'): 1, ('n', 't'): 1, ('t', '</w>'): 4, ('t', 'o'): 3, ('o', '</w>'): 2, ('s', 'e'): 13, ('e', 'a'): 6, ('e', 'e'): 8, ('e', '</w>'): 12, ('w', 'h'): 1, ('h', 'a'): 2, ('a', 't'): 2, ('h', 'e'): 4, ('c', 'o'): 2, ('o', 'u'): 2, ('u', 'l'): 2, ('l', 'd'): 2, ('d', '</w>'): 2, ('b', 'u'): 1, ('u', 't'): 1, ('a', 'l'): 1, ('l', 'l'): 1, ('l', '</w>'): 1, ('t', 'h'): 3, ('w', 'a'): 1, ('a', 's'): 1, ('s', '</w>'): 1, ('b', 'o'): 1, ('o', 't'): 1, ('t', 't'): 1, ('o', 'm'): 1, ('m', '</w>'): 1, ('o', 'f'): 1, ('f', '</w>'): 1, ('d', 'e'): 1, ('e', 'p'): 1, ('p', '</w>'): 1, ('b', 'l'): 1, ('l', 'u'): 1, ('u', 'e'): 1})\n",
      "Number of distinct pairs: 48\n",
      "Most frequent pair: ('s', 'e')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create frequency dict of tokens and only merge most common token\n",
    "Merge the instances of the best pair in the vocabulary\n",
    "\n",
    "<span style=\"color:green;white-space:pre-wrap\">So at first I thought this was worthless and nothing changed, but I doubled checked and saw that actually any word token that has 's' and 'e' as adjacent pair (because we input max frequency pair which happened to be ('s','e'), where they are surrounded by whitespace will actually get merged. So that's the only change happening. The rest of the tokens are the same. What the output of this is the frequencies of the tokens in the vocabulary including the frequency of the new merged token that has 's' 'e' in it. Also, the reason we condition this on the most frequent pair everytime is because merged words are expected to be fewer than the unmerged words, because the vocab started with keys in which each character were padded by whitespaces, so none of them started as words at the initialization of the keys of the vocab (they obviously started as words in the text that the vocab used as input).</span>"
   ],
   "metadata": {
    "id": "pcborzqIXQFS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def merge_pair_in_vocabulary(pair, vocab_in):\n",
    "    vocab_out = {}\n",
    "    bigram = re.escape(' '.join(pair))  # escaping the most frequent pair that is ('s', 'e') and padding white space between them, creating 's \\ e'\n",
    "    # (?<!B)A means negative lookbehind, can think of it as < pointing to behind. It means find expression A whatever where B does not precede \n",
    "    # A(?!B) means negative lookahead, means find expression A where B does not follow\n",
    "    # In this case it means that it matches the bigram that is NOT preceded by a non-whitespace character and NOT followed by a non-whitespace character,\n",
    "    # so essentially we are looking for the bigram that is SURROUNDED by space characters\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in vocab_in:\n",
    "        # in sub re.sub(pattern, replacement, text), we already specified the pattern above, \n",
    "        # so now we want to look for all instances in the word that matches with the pattern \"bigram surrounded by white space\", \n",
    "        # and replace those instances with the actual bigram. The bigram in this case is the most frequent pair, which is 'se' after the join ''.() transformation\n",
    "        # in essence what we are doing is removing the white space surrounding the bigram, \n",
    "        # and we are doing this for all occurences of 's e' and replace them with 'se', i.e. we are merging the bigram\n",
    "        # print(f\"before {word}\")\n",
    "        word_out = p.sub(''.join(pair), word)\n",
    "        # print(f\"after {word_out}\")\n",
    "        # then we associate frequency of the word with the bigram, where bigram is the key and frequency the value\n",
    "        vocab_out[word_out] = vocab_in[word]\n",
    "    return vocab_out"
   ],
   "metadata": {
    "id": "xQI6NALdWQZX",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.923918700Z",
     "start_time": "2024-05-17T02:30:27.865787400Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# This is important to mention, but all variables in jupyter notebook are global variables, so mutating vocab will change it forever, even if I rerun this cell. It will be changed for the duration of the instance.\n",
    "# One way to solve this is to just rerun the entire notebook, especially the part where we reinitialize the vocab.\n",
    "print(vocab)\n",
    "vocab = merge_pair_in_vocabulary(most_frequent_pair, vocab)\n",
    "print('Vocabulary: {}'.format(vocab))\n",
    "print('Size of vocabulary: {}'.format(len(vocab)))"
   ],
   "metadata": {
    "id": "TRYeBZI3ZULu",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.923918700Z",
     "start_time": "2024-05-17T02:30:27.869705Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'a </w>': 1, 's a i l o r </w>': 1, 'w e n t </w>': 1, 't o </w>': 2, 's e a </w>': 6, 's e e </w>': 7, 'w h a t </w>': 1, 'h e </w>': 2, 'c o u l d </w>': 2, 'b u t </w>': 1, 'a l l </w>': 1, 't h a t </w>': 1, 'w a s </w>': 1, 't h e </w>': 2, 'b o t t o m </w>': 1, 'o f </w>': 1, 'd e e p </w>': 1, 'b l u e </w>': 1})\n",
      "Vocabulary: {'a </w>': 1, 's a i l o r </w>': 1, 'w e n t </w>': 1, 't o </w>': 2, 'se a </w>': 6, 'se e </w>': 7, 'w h a t </w>': 1, 'h e </w>': 2, 'c o u l d </w>': 2, 'b u t </w>': 1, 'a l l </w>': 1, 't h a t </w>': 1, 'w a s </w>': 1, 't h e </w>': 2, 'b o t t o m </w>': 1, 'o f </w>': 1, 'd e e p </w>': 1, 'b l u e </w>': 1}\n",
      "Size of vocabulary: 18\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Update the tokens, which now include the best token 'se'"
   ],
   "metadata": {
    "id": "bkhUx3GeXwba"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokens = get_tokens_and_frequencies(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))"
   ],
   "metadata": {
    "id": "Fqj-vQWeXxQi",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.924916800Z",
     "start_time": "2024-05-17T02:30:27.874412800Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: defaultdict(<class 'int'>, {'a': 12, '</w>': 33, 's': 2, 'i': 1, 'l': 6, 'o': 8, 'r': 1, 'w': 3, 'e': 15, 'n': 1, 't': 11, 'se': 13, 'h': 6, 'c': 2, 'u': 4, 'd': 3, 'b': 3, 'm': 1, 'f': 1, 'p': 1})\n",
      "Number of tokens: 20\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's write the full tokenization routine\n",
    "<span style=\"color:green;white-space:pre-wrap\">Note that when the vocab and tokens have converged, but you still want to continue to loop an error will occur on max simply because the vocab will be empty, so get_pairs method will not be able to return a pair dictionary, it will just be empty. This means that when max tries to get something out of the empty dictionary an error will be raised. The reason that we get an empty dictionary is when we have converged there will be no whitespace within the keys of the vocab and thus consequently the pair dictionary. When trying to split each word that is a key that has no white space, the entire word will be returned. In the loop that creates the pairs the length of this list of split characters/words will just be 1, but we are always subtracting that length by 1, which gives zero. So the loop will never run and create pairs. Therefore, an empty dictionary of pairs will be returned. Thus, when we try to take max frequency of this dictionary an error will occur. </span>"
   ],
   "metadata": {
    "id": "K_hKp2kSXXS1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO -- write this routine by filling in this missing parts,\n",
    "# calling the above routines\n",
    "def tokenize(text, num_merges):\n",
    "  # Initialize the vocabulary from the input text\n",
    "  vocab = initialize_vocabulary(text)\n",
    "  # Find the tokens initially and how often they occur in the vocabulary\n",
    "  tokens = get_tokens_and_frequencies(vocab)\n",
    "  \n",
    "  # print initial stats of size of vocab and number of tokens\n",
    "  print(\"Initial stats of size of vocab and number of tokens\")\n",
    "  print('Tokens: {}'.format(tokens))\n",
    "  print('Number of tokens: {}'.format(len(tokens)))\n",
    "  print('Vocabulary: {}'.format(vocab))\n",
    "  print('Size of vocabulary: {}'.format(len(vocab)))\n",
    "  \n",
    "  for i in range(num_merges):\n",
    "    # Find the pairs of adjacent tokens and their counts\n",
    "    pairs = get_pairs_and_counts(vocab)\n",
    "\n",
    "    # Find the most frequent pair\n",
    "    most_frequent_pair = max(pairs, key=pairs.get)\n",
    "    print('Most frequent pair: {}'.format(most_frequent_pair))\n",
    "\n",
    "    # Merge the code in the vocabulary\n",
    "    vocab = merge_pair_in_vocabulary(most_frequent_pair, vocab)\n",
    "\n",
    "  # Find the tokens and how often they occur in the vocabulary one last time\n",
    "  tokens = get_tokens_and_frequencies(vocab)\n",
    "\n",
    "  return tokens, vocab"
   ],
   "metadata": {
    "id": "U_1SkQRGQ8f3",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.924916800Z",
     "start_time": "2024-05-17T02:30:27.881374900Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokens, vocab = tokenize(text, num_merges=50)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('Vocabulary: {}'.format(vocab))\n",
    "print('Size of vocabulary: {}'.format(len(vocab)))"
   ],
   "metadata": {
    "id": "w0EkHTrER_-I",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.925918900Z",
     "start_time": "2024-05-17T02:30:27.885496200Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial stats of size of vocab and number of tokens\n",
      "Tokens: defaultdict(<class 'int'>, {'a': 12, '</w>': 33, 's': 15, 'i': 1, 'l': 6, 'o': 8, 'r': 1, 'w': 3, 'e': 28, 'n': 1, 't': 11, 'h': 6, 'c': 2, 'u': 4, 'd': 3, 'b': 3, 'm': 1, 'f': 1, 'p': 1})\n",
      "Number of tokens: 19\n",
      "Vocabulary: defaultdict(<class 'int'>, {'a </w>': 1, 's a i l o r </w>': 1, 'w e n t </w>': 1, 't o </w>': 2, 's e a </w>': 6, 's e e </w>': 7, 'w h a t </w>': 1, 'h e </w>': 2, 'c o u l d </w>': 2, 'b u t </w>': 1, 'a l l </w>': 1, 't h a t </w>': 1, 'w a s </w>': 1, 't h e </w>': 2, 'b o t t o m </w>': 1, 'o f </w>': 1, 'd e e p </w>': 1, 'b l u e </w>': 1})\n",
      "Size of vocabulary: 18\n",
      "Most frequent pair: ('s', 'e')\n",
      "Most frequent pair: ('e', '</w>')\n",
      "Most frequent pair: ('a', '</w>')\n",
      "Most frequent pair: ('se', 'e</w>')\n",
      "Most frequent pair: ('se', 'a</w>')\n",
      "Most frequent pair: ('t', '</w>')\n",
      "Most frequent pair: ('h', 'e</w>')\n",
      "Most frequent pair: ('t', 'o')\n",
      "Most frequent pair: ('to', '</w>')\n",
      "Most frequent pair: ('h', 'a')\n",
      "Most frequent pair: ('ha', 't</w>')\n",
      "Most frequent pair: ('c', 'o')\n",
      "Most frequent pair: ('co', 'u')\n",
      "Most frequent pair: ('cou', 'l')\n",
      "Most frequent pair: ('coul', 'd')\n",
      "Most frequent pair: ('could', '</w>')\n",
      "Most frequent pair: ('t', 'he</w>')\n",
      "Most frequent pair: ('s', 'a')\n",
      "Most frequent pair: ('sa', 'i')\n",
      "Most frequent pair: ('sai', 'l')\n",
      "Most frequent pair: ('sail', 'o')\n",
      "Most frequent pair: ('sailo', 'r')\n",
      "Most frequent pair: ('sailor', '</w>')\n",
      "Most frequent pair: ('w', 'e')\n",
      "Most frequent pair: ('we', 'n')\n",
      "Most frequent pair: ('wen', 't</w>')\n",
      "Most frequent pair: ('w', 'hat</w>')\n",
      "Most frequent pair: ('b', 'u')\n",
      "Most frequent pair: ('bu', 't</w>')\n",
      "Most frequent pair: ('a', 'l')\n",
      "Most frequent pair: ('al', 'l')\n",
      "Most frequent pair: ('all', '</w>')\n",
      "Most frequent pair: ('t', 'hat</w>')\n",
      "Most frequent pair: ('w', 'a')\n",
      "Most frequent pair: ('wa', 's')\n",
      "Most frequent pair: ('was', '</w>')\n",
      "Most frequent pair: ('b', 'o')\n",
      "Most frequent pair: ('bo', 't')\n",
      "Most frequent pair: ('bot', 'to')\n",
      "Most frequent pair: ('botto', 'm')\n",
      "Most frequent pair: ('bottom', '</w>')\n",
      "Most frequent pair: ('o', 'f')\n",
      "Most frequent pair: ('of', '</w>')\n",
      "Most frequent pair: ('d', 'e')\n",
      "Most frequent pair: ('de', 'e')\n",
      "Most frequent pair: ('dee', 'p')\n",
      "Most frequent pair: ('deep', '</w>')\n",
      "Most frequent pair: ('b', 'l')\n",
      "Most frequent pair: ('bl', 'u')\n",
      "Most frequent pair: ('blu', 'e</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'a</w>': 1, 'sailor</w>': 1, 'went</w>': 1, 'to</w>': 2, 'sea</w>': 6, 'see</w>': 7, 'what</w>': 1, 'he</w>': 2, 'could</w>': 2, 'but</w>': 1, 'all</w>': 1, 'that</w>': 1, 'was</w>': 1, 'the</w>': 2, 'bottom</w>': 1, 'of</w>': 1, 'deep</w>': 1, 'blue</w>': 1})\n",
      "Number of tokens: 18\n",
      "Vocabulary: {'a</w>': 1, 'sailor</w>': 1, 'went</w>': 1, 'to</w>': 2, 'sea</w>': 6, 'see</w>': 7, 'what</w>': 1, 'he</w>': 2, 'could</w>': 2, 'but</w>': 1, 'all</w>': 1, 'that</w>': 1, 'was</w>': 1, 'the</w>': 2, 'bottom</w>': 1, 'of</w>': 1, 'deep</w>': 1, 'blue</w>': 1}\n",
      "Size of vocabulary: 18\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green;white-space:pre-wrap\">It differs initially by one because there is one key that contains only white space, but the vocab does not have white space token (key) because when constructing pairs split is used and it will remove all white spaces within a key from the vocab. It seems to converge to 18, which is expected, because of how the vocab and tokens are constructed.</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO - Consider the input text:\n",
    "\n",
    "\"How much wood could a woodchuck chuck if a woodchuck could chuck wood\"\n",
    "\n",
    "How many tokens will there be initially and what will they be?\n",
    "<span style=\"color:green;white-space:pre-wrap\">Initially the tokens are letters including the white space token. So there are 14 different latters in the text if we also include white space as distinct character, while the vocab will just capture the distinct words with the only difference that they are padded with space and some kind of word tag </w>, but in this case since there are 8 distinct words, the size of vocab is therefore 8.</span>\n",
    "How many tokens will there be if we run the tokenization routine for the maximum number of iterations (merges)?\n",
    "<span style=\"color:green;white-space:pre-wrap\">When it converges (24 steps) the number of tokens should be the same i.e. 8, it's just that each token will now be an actual word.</span>\n",
    "Why does this algorithm work?\n",
    "<span style=\"color:green;white-space:pre-wrap\">The gist of it is that it merges the most common adjacent pair each time given the current vocab content e.g. all occurrences of 's e' become 'se', but if a token is just a whitespace it will not merge with another token/character/subword/word that ends or starts at whitespace because otherwise it would merge across words, which we want to avoid. The only criteria to merge is that there is a subword that is surrounded by white space, and we check every relevant subword in the text. So, eventually all subwords within a key (in the vocab) will be merged, because we gradually are removing whitespaces from them. As mentioned the error after convergence occurs because the pairs dictionary will be empty when all words have been merged, because there won't be any white spaces to split, therefore the loop that creates the pairs will never run. So when we try to take max frequency of pairs, we are essentially trying to access an empty dictionary which raises an error.</span>\n",
    "\n",
    "When you've made your predictions, run the code and see if you are correct."
   ],
   "metadata": {
    "id": "jOW_HJtMdAxd"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial stats of size of vocab and number of tokens\n",
      "Tokens: defaultdict(<class 'int'>, {'H': 1, 'o': 11, 'w': 5, '</w>': 13, 'm': 1, 'u': 7, 'c': 11, 'h': 5, 'd': 6, 'l': 2, 'a': 2, 'k': 4, 'i': 1, 'f': 1})\n",
      "Number of tokens: 14\n",
      "Vocabulary: defaultdict(<class 'int'>, {'H o w </w>': 1, 'm u c h </w>': 1, 'w o o d </w>': 2, 'c o u l d </w>': 2, 'a </w>': 2, 'w o o d c h u c k </w>': 2, 'c h u c k </w>': 2, 'i f </w>': 1})\n",
      "Size of vocabulary: 8\n",
      "Most frequent pair: ('u', 'c')\n",
      "Most frequent pair: ('w', 'o')\n",
      "Most frequent pair: ('wo', 'o')\n",
      "Most frequent pair: ('woo', 'd')\n",
      "Most frequent pair: ('c', 'h')\n",
      "Most frequent pair: ('ch', 'uc')\n",
      "Most frequent pair: ('chuc', 'k')\n",
      "Most frequent pair: ('chuck', '</w>')\n",
      "Most frequent pair: ('wood', '</w>')\n",
      "Most frequent pair: ('c', 'o')\n",
      "Most frequent pair: ('co', 'u')\n",
      "Most frequent pair: ('cou', 'l')\n",
      "Most frequent pair: ('coul', 'd')\n",
      "Most frequent pair: ('could', '</w>')\n",
      "Most frequent pair: ('a', '</w>')\n",
      "Most frequent pair: ('wood', 'chuck</w>')\n",
      "Most frequent pair: ('H', 'o')\n",
      "Most frequent pair: ('Ho', 'w')\n",
      "Most frequent pair: ('How', '</w>')\n",
      "Most frequent pair: ('m', 'uc')\n",
      "Most frequent pair: ('muc', 'h')\n",
      "Most frequent pair: ('much', '</w>')\n",
      "Most frequent pair: ('i', 'f')\n",
      "Most frequent pair: ('if', '</w>')\n",
      "Tokens: defaultdict(<class 'int'>, {'How</w>': 1, 'much</w>': 1, 'wood</w>': 2, 'could</w>': 2, 'a</w>': 2, 'woodchuck</w>': 2, 'chuck</w>': 2, 'if</w>': 1})\n",
      "Number of tokens: 8\n",
      "Vocabulary: {'How</w>': 1, 'much</w>': 1, 'wood</w>': 2, 'could</w>': 2, 'a</w>': 2, 'woodchuck</w>': 2, 'chuck</w>': 2, 'if</w>': 1}\n",
      "Size of vocabulary: 8\n"
     ]
    }
   ],
   "source": [
    "woody_text = \"How much wood could a woodchuck chuck if a woodchuck could chuck wood\"\n",
    "tokens, vocab = tokenize(woody_text, num_merges=24)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('Vocabulary: {}'.format(vocab))\n",
    "print('Size of vocabulary: {}'.format(len(vocab)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.925918900Z",
     "start_time": "2024-05-17T02:30:27.896656600Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:27.976918300Z",
     "start_time": "2024-05-17T02:30:27.901909300Z"
    }
   },
   "execution_count": 14
  }
 ]
}
