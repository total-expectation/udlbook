{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOKrX9gmuhl9+KwscpZKr3u",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap12/12_1_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Notebook 12.1: Self Attention**\n",
    "\n",
    "This notebook builds a self-attention mechanism from scratch, as discussed in section 12.2 of the book.\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
    "\n",
    "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
    "\n"
   ],
   "metadata": {
    "id": "t9vk9Elugvmi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ],
   "metadata": {
    "id": "OLComQyvCIJ7",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.592500800Z",
     "start_time": "2024-05-17T02:30:21.997959600Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
    "\n"
   ],
   "metadata": {
    "id": "9OJkkoNqCVK2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(3)\n",
    "# Number of inputs\n",
    "N = 3\n",
    "# Number of dimensions of each input\n",
    "D = 4\n",
    "# Create an empty list\n",
    "all_x = []\n",
    "# Create elements x_n and append to list\n",
    "for n in range(N):\n",
    "  all_x.append(np.random.normal(size=(D,1)))\n",
    "# Print out the list\n",
    "print(all_x)\n"
   ],
   "metadata": {
    "id": "oAygJwLiCSri",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.598498700Z",
     "start_time": "2024-05-17T02:30:22.594501500Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.78862847],\n",
      "       [ 0.43650985],\n",
      "       [ 0.09649747],\n",
      "       [-1.8634927 ]]), array([[-0.2773882 ],\n",
      "       [-0.35475898],\n",
      "       [-0.08274148],\n",
      "       [-0.62700068]]), array([[-0.04381817],\n",
      "       [-0.47721803],\n",
      "       [-1.31386475],\n",
      "       [ 0.88462238]])]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"
   ],
   "metadata": {
    "id": "W2iHFbtKMaDp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose random values for the parameters\n",
    "omega_q = np.random.normal(size=(D,D))\n",
    "omega_k = np.random.normal(size=(D,D))\n",
    "omega_v = np.random.normal(size=(D,D))\n",
    "beta_q = np.random.normal(size=(D,1))\n",
    "beta_k = np.random.normal(size=(D,1))\n",
    "beta_v = np.random.normal(size=(D,1))"
   ],
   "metadata": {
    "id": "79TSK7oLMobe",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.603340800Z",
     "start_time": "2024-05-17T02:30:22.599498600Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's compute the queries, keys, and values for each input"
   ],
   "metadata": {
    "id": "VxaKQtP3Ng6R"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![attention 12.2](../../public/attention122.png)\n",
    "![attention 12.4](../../public/attention124.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Make three lists to store queries, keys, and values\n",
    "all_queries = []\n",
    "all_keys = []\n",
    "all_values = []\n",
    "# For every input\n",
    "# all_x is 4x3, i.e 4 dimensional with 3 samples in total, each col is a sample\n",
    "all_x2 = np.zeros((D, N))  # 4x3 input matrix, 4 dimen and 3 samples\n",
    "for idx, x in enumerate(all_x):\n",
    "  all_x2[:, idx] = x[:,0]\n",
    "\n",
    "# matrix multiplication method\n",
    "all_queries2 = beta_q + np.dot(omega_q, all_x2)\n",
    "all_keys2 = beta_k + np.dot(omega_k, all_x2)\n",
    "all_values2 = beta_v + np.dot(omega_v, all_x2)\n",
    "\n",
    "for x in all_x:\n",
    "  # TODO -- compute the keys, queries and values.\n",
    "  # Replace these three lines\n",
    "  # these are 3x4, i.e each row is the result of weights * a_sample\n",
    "  query = beta_q + np.dot(omega_q, x)\n",
    "  key = beta_k + np.dot(omega_k, x)\n",
    "  value = beta_v + np.dot(omega_v, x)\n",
    "\n",
    "  all_queries.append(query)\n",
    "  all_keys.append(key)\n",
    "  all_values.append(value)\n",
    "\n",
    "aq = np.array([a[:,0] for a in all_queries]).T\n",
    "ak = np.array([a[:,0] for a in all_keys]).T\n",
    "av = np.array([a[:,0] for a in all_values]).T\n",
    "print(f\"This shows there are rounding errors, so using array_equals or (==).all() will not work as that will just return false. Use np.isclose().all() or np.allclose()\\n{aq - all_queries2}\\n{ak - all_keys2}\\n{av - all_values2}\")\n",
    "\n",
    "assert np.allclose(aq, all_queries2)\n",
    "assert np.allclose(ak, all_keys2)\n",
    "assert np.allclose(av, all_values2)"
   ],
   "metadata": {
    "id": "TwDK2tfdNmw9",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.611794700Z",
     "start_time": "2024-05-17T02:30:22.606340700Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This shows there are rounding errors, so using array_equals or (==).all() will not work as that will just return false. Use np.isclose().all() or np.allclose()\n",
      "[[ 0.00000000e+00 -4.44089210e-16  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.11022302e-16  0.00000000e+00]]\n",
      "[[ 0.00000000e+00  5.55111512e-17  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 8.88178420e-16 -1.04083409e-17  5.55111512e-17]\n",
      " [ 0.00000000e+00  2.22044605e-16  1.11022302e-16]]\n",
      "[[ 4.44089210e-16  0.00000000e+00  2.22044605e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00 -5.55111512e-17]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll need a softmax function (equation 12.5) -- here, it will take a list of arbitrary numbers and return a list where the elements are non-negative and sum to one\n"
   ],
   "metadata": {
    "id": "Se7DK6PGPSUk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![softmax 12.5](../../public/notebook125prince.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def softmax(k_m, q_n, items_in):\n",
    "\n",
    "  # TODO Compute the elements of items_out\n",
    "  # Replace this line\n",
    "  items_out = np.exp(np.dot(k_m[:, 0], q_n[:, 0])) / np.sum(np.exp(items_in))\n",
    "\n",
    "  return items_out"
   ],
   "metadata": {
    "id": "u93LIcE5PoiM",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.627934700Z",
     "start_time": "2024-05-17T02:30:22.614793200Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now compute the self attention values:"
   ],
   "metadata": {
    "id": "8aJVhbKDW7lm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![softmax 12.5](../../public/notebook125prince.png)\n",
    "![self attention 12.3](../../public/notebook123prince.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create emptymlist for output\n",
    "all_x_prime = []\n",
    "\n",
    "# For each output\n",
    "# Create list for dot products of query N with all keys\n",
    "# Note that there is alot of unecessary loops because we aren't vectorizing\n",
    "for n in range(N):\n",
    "  # Compute the dot products, this corresponds to all the dot products needed in the denominator in 12.5\n",
    "  all_dots_km_over_given_qn = [np.dot(key[:, 0], all_queries[n][:, 0]) for key in all_keys]\n",
    "  # Compute attention, but I think Prince wants kmqn/normalizing_constant over all m, that way we get a vector and can check that it sums to one. This is just a sanity check to check that the softmax method works\n",
    "  attention = [softmax(all_keys[m], all_queries[n], all_dots_km_over_given_qn) for m in range(N)]\n",
    "  # Print result (should be positive sum to one)\n",
    "  print(\"Attentions for output \", n)\n",
    "  print(attention)\n",
    "  print(f\"Check that it sums to one: {np.sum(attention)}\")\n",
    "  # TODO: Compute a weighted sum of all of the values according to the attention\n",
    "  # (equation 12.3)\n",
    "  # Replace this line\n",
    "  # This line computes the attention and the weighted sum with the computed attention at the same time\n",
    "  all_x_prime.append(sum([softmax(all_keys[m], all_queries[n], all_dots_km_over_given_qn) * all_values[m] for m in range(N)]))\n",
    "\n",
    "# Print out true values to check you have it correct\n",
    "print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n",
    "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
    "print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n",
    "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
    "print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n",
    "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")\n"
   ],
   "metadata": {
    "id": "yimz-5nCW6vQ",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.628455Z",
     "start_time": "2024-05-17T02:30:22.619536600Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attentions for output  0\n",
      "[1.2432614615724094e-13, 0.9982814887008525, 0.0017185112990231368]\n",
      "Check that it sums to one: 1.0\n",
      "Attentions for output  1\n",
      "[2.7952530620087617e-12, 0.0058550635983758564, 0.994144936398829]\n",
      "Check that it sums to one: 1.0\n",
      "Attentions for output  2\n",
      "[0.005057079072941125, 0.006547760717181933, 0.9883951602098769]\n",
      "Check that it sums to one: 1.0\n",
      "x_prime_0_calculated: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
      "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
      "x_prime_1_calculated: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
      "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
      "x_prime_2_calculated: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
      "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's compute the same thing, but using matrix calculations.  We'll store the $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ in the columns of a $D\\times N$ matrix, using equations 12.6 and 12.7/8.\n",
    "\n",
    "Note:  The book uses column vectors (for compatibility with the rest of the text), but in the wider literature it is more normal to store the inputs in the rows of a matrix;  in this case, the computation is the same, but all the matrices are transposed and the operations proceed in the reverse order."
   ],
   "metadata": {
    "id": "PJ2vCQ_7C38K"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![12.6 and 12.7](../../public/attention126127prince.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "  # Exponentiate all the values\n",
    "  # this should be K^T * Q i.e. if K and Q are DxD both, then we get DxD matrix\n",
    "  exp_values = np.exp(data_in)\n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0)\n",
    "  # Replicate denominator to N rows, this is an outer product\n",
    "  # However numpy knows broadcasting, so it would work without having to undergo this process\n",
    "  # I guess the author does this for pedagogical reasons\n",
    "  # denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
    "  # Compute softmax\n",
    "  # Normalize column-wise because of how K^TQ fills the matrix, going by the formulas 12.5 and 12.3 of how the matrix are filled (write it out),\n",
    "  # the matrix formulation is ultimately filled in the same way in the end, so then it will be clear why we want to normalize the columns\n",
    "  softmax = exp_values / denom\n",
    "  # return the answer\n",
    "  return softmax"
   ],
   "metadata": {
    "id": "obaQBdUAMXXv",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.632872400Z",
     "start_time": "2024-05-17T02:30:22.626715400Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    " # Now let's compute self attention in matrix form\n",
    "def self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "\n",
    "  # TODO -- Write this function\n",
    "  # 1. Compute queries, keys, and values\n",
    "  # 2. Compute dot products\n",
    "  # 3. Apply softmax to calculate attentions\n",
    "  # 4. Weight values by attentions\n",
    "  # Replace this line\n",
    "  Q = beta_q + np.dot(omega_q, X)\n",
    "  K = beta_k + np.dot(omega_k, X)\n",
    "  V = beta_v + np.dot(omega_v, X)\n",
    "  attention = softmax_cols(np.dot(K.T, Q))\n",
    "  X_prime = np.dot(V, attention)\n",
    "\n",
    "\n",
    "  return X_prime, attention"
   ],
   "metadata": {
    "id": "gb2WvQ3SiH8r",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.635388Z",
     "start_time": "2024-05-17T02:30:22.631452Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Copy data into matrix\n",
    "X = np.zeros((D, N))\n",
    "X[:,0] = np.squeeze(all_x[0])\n",
    "X[:,1] = np.squeeze(all_x[1])\n",
    "X[:,2] = np.squeeze(all_x[2])\n",
    "\n",
    "# Run the self attention mechanism\n",
    "X_prime, attention_matrix = self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime.T)\n",
    "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
    "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
    "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")"
   ],
   "metadata": {
    "id": "MUOJbgJskUpl",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.641057Z",
     "start_time": "2024-05-17T02:30:22.637390700Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]\n",
      " [ 1.64201168 -0.08470004  4.02764044  2.18690791]\n",
      " [ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
      "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
      "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
      "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you did this correctly, the values should be the same as above.\n",
    "\n",
    "TODO:  \n",
    "\n",
    "Print out the attention matrix\n",
    "You will see that the values are quite extreme (one is very close to one and the others are very close to zero.  Now we'll fix this problem by using scaled dot-product attention."
   ],
   "metadata": {
    "id": "as_lRKQFpvz0"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.24326146e-13 2.79525306e-12 5.05707907e-03]\n",
      " [9.98281489e-01 5.85506360e-03 6.54776072e-03]\n",
      " [1.71851130e-03 9.94144936e-01 9.88395160e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(attention_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.645965300Z",
     "start_time": "2024-05-17T02:30:22.642057300Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green;white-space:pre-wrap\">This is something I noticed a few were much higher, even very close to 1 than the others. Because of this the rest of the probabilities gets heavily diluted to the point of almost being zero.</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![12.9](../../public/attention129prince.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Now let's compute self attention in matrix form\n",
    "def scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "\n",
    "  # TODO -- Write this function\n",
    "  # 1. Compute queries, keys, and values\n",
    "  # 2. Compute dot products\n",
    "  # 3. Scale the dot products as in equation 12.9\n",
    "  # 4. Apply softmax to calculate attentions\n",
    "  # 5. Weight values by attentions\n",
    "  # Replace this line\n",
    "  Q = beta_q + omega_q @ X\n",
    "  K = beta_k + omega_k @ X\n",
    "  V = beta_v + omega_v @ X\n",
    "  row_in_query_and_key = Q.shape[0]\n",
    "  attention = softmax_cols((K.T @ Q) / np.sqrt(row_in_query_and_key))\n",
    "  X_prime = V @ attention\n",
    "\n",
    "  return X_prime"
   ],
   "metadata": {
    "id": "kLU7PUnnqvIh",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.651693700Z",
     "start_time": "2024-05-17T02:30:22.646965100Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run the self attention mechanism\n",
    "X_prime = scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime.T)\n",
    "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
    "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
    "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")"
   ],
   "metadata": {
    "id": "n18e3XNzmVgL",
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.670529700Z",
     "start_time": "2024-05-17T02:30:22.650696900Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.97411966 -0.23738409 -0.72333202 -0.34413007]\n",
      " [ 1.59622051 -0.09516106  3.70194096  2.01339538]\n",
      " [ 1.32638014  0.13062402  3.02371664  1.6902419 ]]\n",
      "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
      "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
      "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green;white-space:pre-wrap\">It's expected of the values to be different now simply because we have scaled them.</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO -- Investigate whether the self-attention mechanism is covariant with respect to permutation.\n",
    "If it is, when we permute the columns of the input matrix $\\mathbf{X}$, the columns of the output matrix $\\mathbf{X}'$ will also be permuted.\n"
   ],
   "metadata": {
    "id": "QDEkIrcgrql-"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.64201168 -0.08470004  4.02764044  2.18690791]\n",
      " [ 0.94744244 -0.24348429 -0.91310441 -0.44522983]\n",
      " [ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
      "\n",
      "True answers for the original matrix, i.e. the non-permutated matrix\n",
      "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
      "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
      "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"
     ]
    }
   ],
   "source": [
    "# X_permuted = X[:, np.random.permutation(X.shape[1])]  # not reliable as too few samples, so there might not even be any switching of cols\n",
    "X_permuted = X[:, [1,0,2]]\n",
    "# Run the self attention mechanism\n",
    "X_prime, attention_matrix = self_attention(X_permuted,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime.T)\n",
    "print(\"\\nTrue answers for the original matrix, i.e. the non-permutated matrix\")\n",
    "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
    "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
    "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T02:30:22.671529700Z",
     "start_time": "2024-05-17T02:30:22.656230200Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:green;white-space:pre-wrap\">As can be seen the permutation is actually reflected: switching the first and second columns in input data switches the first and second column in the output matrix (we have transposed this to rows to match the solution text), one could easily show this mathematically by just explicitly inspecting the columns or rows, and track what happens at the output matrix.</span>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
